{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization of a Large Language Model (LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from a .env file\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Set the project root directory and update the system path\n",
    "project_root_directory = os.getcwd().split(\"notebooks\")[0]\n",
    "sys.path.insert(0, project_root_directory)\n",
    "notebook_path = os.path.join(project_root_directory, \"notebooks\")\n",
    "sys.path.insert(0, notebook_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1727264749255
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import (\n",
    "    DefaultAzureCredential,\n",
    "    InteractiveBrowserCredential,\n",
    ")\n",
    "from azure.ai.ml.entities import AmlCompute\n",
    "import time\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "try:\n",
    "    workspace_ml_client = MLClient.from_config(credential=credential)\n",
    "except:\n",
    "    workspace_ml_client = MLClient(\n",
    "        credential,\n",
    "        subscription_id=os.environ[\"SUBSCRIPTION_ID\"],\n",
    "        resource_group_name=os.environ[\"RESOURCE_GROUP\"],\n",
    "        workspace_name=os.environ[\"WORKSPACE_NAME\"],\n",
    "    )\n",
    "\n",
    "# the models, fine tuning pipelines and environments are available in the AzureML system registry, \"azureml\"\n",
    "registry_ml_client = MLClient(credential, registry_name=\"azureml\")\n",
    "\n",
    "# generating a unique timestamp that can be used for names and versions that need to be unique\n",
    "timestamp = str(int(time.time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Quantization of a Model from Marketplace/Hugging Face \n",
    "\n",
    "When using a PEFT (Parameter-Efficient Fine-Tuning) model, it is essential to utilize the `convert_lora.py` script. This script is specifically designed to handle the conversion and quantization of PEFT models, ensuring optimal performance and efficiency.\n",
    "\n",
    "- First step is to clone the llama.cpp repo, because we need it to use the methods within.\n",
    "- More info about build: [llama.cpp build documentation](https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core.quantize_model import clone_repo, convert_model_to_gguf, quantize_model, build_project\n",
    "clone_repo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_project()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the model - from your personal registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dir = \"./model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Model\n",
    "\n",
    "# Connect to Azure ML workspace\n",
    "workspace = Workspace.from_config()\n",
    "\n",
    "# Specify the model name you want to download\n",
    "model_name = \"registered_model\"\n",
    "\n",
    "# Get the model\n",
    "model = Model(workspace, name=model_name, version=\"x\")\n",
    "\n",
    "# Download the model\n",
    "model.download(target_dir=local_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Model {model_name} downloaded to {local_dir} directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Hugging Face Library\n",
    "\n",
    "- Alternatively, we can download the model directly using the Hugging Face library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "TOKEN = os.environ.get(\"TOKEN\")\n",
    "base_model_id = \"teknium/OpenHermes-2.5-Mistral-7B\"\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=base_model_id, local_dir=local_dir, token=TOKEN\n",
    ")\n",
    "# or using component import_model = registry_ml_client.components.get(name=\"download_model\", label=\"latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"t5-large\"\n",
    "foundation_model = registry_ml_client.models.get(model_name, label=\"latest\")\n",
    "print(\n",
    "    \"\\n\\nUsing model name: {0}, version: {1}, id: {2} for fine tuning\".format(\n",
    "        foundation_model.name, foundation_model.version, foundation_model.id\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Convert hf to ggu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"original_model\"\n",
    "original_model_path = \"./model/\"\n",
    "#original_model_path = \"./model/mlflow_model_folder/data/model/\"\n",
    "quantized_model_path = \"./model_quantized/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_model_to_gguf(original_model_path, quantized_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_path = f\"{project_root_directory}notebooks//model_quantized\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_path = quantize_model(quantized_model_path, \"q4_k_m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - How Does the Base Model Perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dir = \"./model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dir or base_model_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(local_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(local_dir)\n",
    "pipeline = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer, task=\"text-generation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the model's properties\n",
    "print(\"Model's device:\", model.device)\n",
    "print(\"Model's dtype:\", model.dtype)\n",
    "print(\"Model's max lenght:\", tokenizer.model_max_length)\n",
    "print(\"Model's parameters:\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"  {name}: {param.shape}, {param.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only input table and question, since system prompt is adeed in the prompt template.\n",
    "my_prompt = \"Summarize this dialog:\\nHannah: Hey, do you have Betty's number?\\nAmanda: Lemme check\\nHannah: <file_gif>\\nAmanda: Sorry, can't find it.\\nAmanda: Ask Larry\\nAmanda: He called her last time we were at the park together\\nHannah: I don't know him well\\nHannah: <file_gif>\\nAmanda: Don't be shy, he's very nice\\nHannah: If you say so..\\nHannah: I'd rather you texted him\\nAmanda: Just text him ðŸ™‚\\nHannah: Urgh.. Alright\\nHannah: Bye\\nAmanda: Bye bye\\n---\\nSummary:\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure the latency\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    response = pipeline(\n",
    "        my_prompt,\n",
    "        max_new_tokens=256,\n",
    "        repetition_penalty=1.15,\n",
    "        return_full_text=False,\n",
    "        pad_token_id=tokenizer.pad_token_id,  # Ensure this is an integer\n",
    "    )\n",
    "\n",
    "end_time = time.time()\n",
    "latency = end_time - start_time\n",
    "\n",
    "# Calculate the number of tokens generated\n",
    "generated_text = response[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Quantization and Deployment of a Fine-Tuned Large Language Model (LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# GLOBAL VARIABLES\n",
    "my_model_path = \"./model_quantized/Q4_K_M.gguf\"\n",
    "CONTEXT_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LOAD THE MODEL\n",
    "model_quantized = Llama(model_path=my_model_path, n_ctx=CONTEXT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_from_prompt(\n",
    "    user_prompt, max_tokens=256, temperature=0.3, top_p=0.1, echo=True, stop=[\"Q\", \"\\n\"]\n",
    "):\n",
    "\n",
    "    # Define the parameters\n",
    "    model_output = model_quantized(\n",
    "        user_prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        echo=echo,\n",
    "        stop=stop,\n",
    "    )\n",
    "\n",
    "    return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = generate_text_from_prompt(my_prompt)\n",
    "\n",
    "print(model_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response[\"choices\"][0][\"text\"].strip().split(\"Summary:\")[1].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Model to MLflow Template\n",
    "\n",
    "We can convert this model into an MLflow template for easier deployment and reproducibility. Below are the steps to achieve this:\n",
    "\n",
    "1. **Create a Custom Loader\n",
    "2. **Prepare the Model**: Ensure the model is in the correct format and directory.\n",
    "3. **Log the Model**: Use MLflow to log the model with the appropriate signature and parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model_path = \"./model_quantized/Q4_K_M.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core.tracking_model import tracking_mlflow_model \n",
    "\n",
    "code_path = \"/home/azureuser/cloudfiles/code/Users/karinaa/fine-tuning-text-to-sql/src/core/custom_loader\"\n",
    "conda_path = \"conda.yaml\"\n",
    "tracking_mlflow_model(code_path, my_model_path, conda_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Logged Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your run ID from MLflow\n",
    "\n",
    "run_id = \"23ca6ac6-9191-4854-802c-c9cb72689864\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_model = mlflow.pyfunc.load_model(f\"runs:/{run_id}/model\")\n",
    "latest_run = mlflow.search_runs(order_by=[\"start_time desc\"]).iloc[0]\n",
    "print(f\"Latest run ID: {latest_run.run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a json object with the key as \"input_data\" and value as a list of values from the text column of the test dataframe\n",
    "data = {\"text\": [my_prompt]}\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "unwrapped_model = mlflow_model.unwrap_python_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = unwrapped_model.predict(data, {'max_tokens': 256})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    ProbeSettings,\n",
    "    OnlineRequestSettings,\n",
    ")\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "endpoint_name = \"endpt-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n",
    "\n",
    "# create an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=endpoint_name,\n",
    "    description=\"Online endpoint for fine tuned  quantized model\",\n",
    "    auth_mode=\"key\",\n",
    ")\n",
    "workspace_ml_client.begin_create_or_update(endpoint).wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find here the list of SKU's supported for deployment - [Managed online endpoints SKU list](https://learn.microsoft.com/en-us/azure/machine-learning/reference-managed-online-endpoints-vm-sku-list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model_name = \"quantized_model\"\n",
    "version = \"2\"\n",
    "\n",
    "registered_model = workspace_ml_client.models.get(\n",
    "    name=finetuned_model_name, version=version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Specify the environment name and version you want to retrieve\n",
    "environment_name = \"inferencing-env\"\n",
    "environment_version = \"1\"\n",
    "\n",
    "# Get the environment\n",
    "environment = workspace_ml_client.environments.get(name=environment_name, version=environment_version)\n",
    "\n",
    "environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deployment\n",
    "demo_deployment = ManagedOnlineDeployment(\n",
    "    name=\"blue\",\n",
    "    endpoint_name=\"endpt-10291635182376\",\n",
    "    model=registered_model.id,\n",
    "    instance_type=\"Standard_NC48ads_A100_v4\",  # use GPU instance type for faster explanations\n",
    "    instance_count=1,\n",
    "    #environment=environment,\n",
    "    request_settings=OnlineRequestSettings(\n",
    "        max_concurrent_requests_per_instance=1,\n",
    "        request_timeout_ms=90000,\n",
    "        max_queue_wait_ms=500,\n",
    "    ),\n",
    "    liveness_probe=ProbeSettings(\n",
    "        failure_threshold=49,\n",
    "        success_threshold=1,\n",
    "        timeout=299,\n",
    "        period=180,\n",
    "        initial_delay=180,\n",
    "    ),\n",
    "    readiness_probe=ProbeSettings(\n",
    "        failure_threshold=10,\n",
    "        success_threshold=1,\n",
    "        timeout=10,\n",
    "        period=10,\n",
    "        initial_delay=2000,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_ml_client.online_deployments.begin_create_or_update(demo_deployment).wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.traffic = {\"blue\": 100}\n",
    "workspace_ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the endpoint with sample data\n",
    "\n",
    "We will fetch some sample data from the test dataset and submit to online endpoint for inference. We will then show the display the scored labels alongside the ground truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "data = [\n",
    "    \"Summarize this dialog:\\nHannah: Hey, do you have Betty's number?\\nAmanda: Lemme check\\nHannah: <file_gif>\\nAmanda: Sorry, can't find it.\\nAmanda: Ask Larry\\nAmanda: He called her last time we were at the park together\\nHannah: I don't know him well\\nHannah: <file_gif>\\nAmanda: Don't be shy, he's very nice\\nHannah: If you say so..\\nHannah: I'd rather you texted him\\nAmanda: Just text him ðŸ™‚\\nHannah: Urgh.. Alright\\nHannah: Bye\\nAmanda: Bye bye\\n---\\nSummary:\\n\"\n",
    "]\n",
    "\n",
    "# Constructing the required JSON-like structure\n",
    "input_data = {\n",
    "    \"input_data\": {\n",
    "        \"index\": [0],\n",
    "        \"columns\": [\"text\"],\n",
    "        \"data\": [[dialog] for dialog in data]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Output the result\n",
    "print(input_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save the json object to a file named sample_score.json in the ./samsum-dataset folder\n",
    "with open(\"sample_score.json\", \"w\") as f:\n",
    "    json.dump(input_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_endpoint_name = \"endpt-10291635182376\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score the sample_score.json file using the online endpoint with the azureml endpoint invoke method\n",
    "response = workspace_ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    deployment_name=\"blue\",\n",
    "    request_file=\"sample_score.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the response to a pandas dataframe and rename the label column as scored_label\n",
    "response_df = pd.read_json(response)\n",
    "response_df = response_df.rename(columns={0: \"scored_label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_ml_client.online_endpoints.begin_delete(name=online_endpoint_name).wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "# Delete the model_download folder if it exists\n",
    "if os.path.exists(original_model_path):\n",
    "    shutil.rmtree(original_model_path)\n",
    "\n",
    "# Delete the model_download folder if it exists\n",
    "if os.path.exists(\"llama.cpp\"):\n",
    "    shutil.rmtree(\"llama.cpp\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "PEFT Llama Finetuning",
   "widgets": {}
  },
  "kernel_info": {
   "name": "condav0"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "microsoft": {
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
